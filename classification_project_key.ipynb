{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_project_key.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CxDMbHJG9Qpg"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmarogge/cs280r/blob/master/classification_project_key.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyMdovPAKAHU",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2: Text classification with Colab and PyTorch\n",
        "\n",
        "Emma Rogge, Tasha Schoenstein & Zilin Ma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKCmwuSH0acV",
        "colab_type": "text"
      },
      "source": [
        "## Set up\n",
        "\n",
        "###Import relevant libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdi-spgB0sEi",
        "colab_type": "code",
        "outputId": "4abc461a-7bee-4285-8c3a-2b8b78024c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchtext import data\n",
        "import math\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "## GPU check, make sure to set runtime type to \"GPU\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGVWcvlk080Q",
        "colab_type": "code",
        "outputId": "b5402dfb-4707-41d4-c6bd-604ae4596d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "source": [
        "## Read in data files\n",
        "!wget https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/train.nl\n",
        "!wget https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/train.sql\n",
        "!wget https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/test.nl\n",
        "!wget https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/test.sql"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 16:25:57--  https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/train.nl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 281581 (275K) [text/plain]\n",
            "Saving to: ‘train.nl’\n",
            "\n",
            "\rtrain.nl              0%[                    ]       0  --.-KB/s               \rtrain.nl            100%[===================>] 274.98K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-12-07 16:25:57 (9.43 MB/s) - ‘train.nl’ saved [281581/281581]\n",
            "\n",
            "--2019-12-07 16:25:58--  https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/train.sql\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2840349 (2.7M) [text/plain]\n",
            "Saving to: ‘train.sql’\n",
            "\n",
            "train.sql           100%[===================>]   2.71M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-12-07 16:25:58 (45.4 MB/s) - ‘train.sql’ saved [2840349/2840349]\n",
            "\n",
            "--2019-12-07 16:25:59--  https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/test.nl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24646 (24K) [text/plain]\n",
            "Saving to: ‘test.nl’\n",
            "\n",
            "test.nl             100%[===================>]  24.07K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2019-12-07 16:25:59 (3.23 MB/s) - ‘test.nl’ saved [24646/24646]\n",
            "\n",
            "--2019-12-07 16:26:00--  https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/test.sql\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 250477 (245K) [text/plain]\n",
            "Saving to: ‘test.sql’\n",
            "\n",
            "test.sql            100%[===================>] 244.61K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-12-07 16:26:00 (8.27 MB/s) - ‘test.sql’ saved [250477/250477]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPJ6ihGH1Oz8",
        "colab_type": "text"
      },
      "source": [
        "##Data format\n",
        "\n",
        "We're going to use `torchtext` to handle processing the data. This library is useful for processing and batching text data in Python. More information on `torchtext` can be found [in this tutorial](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPdPKDzo-Vf6",
        "colab_type": "text"
      },
      "source": [
        "###Implement torchtext Dataset\n",
        "Implement the class below to prepare the data for classification.\n",
        "\n",
        "#### Hints:\n",
        "- Consider utilizing PyTorch's `preprocess` method on your text field objects--this method tokenizes the content of a text field object using `spacy`.\n",
        "- Start by populating a list with each processed, tokenized query in your dataset.\n",
        "- Each text field object in your list should then have its `label` field populated with the appropriate label.\n",
        "- Leverage the `__init__` method of the parent class (`pytorch.utils.data.Dataset`).\n",
        "\n",
        "---\n",
        "### Bag-of-Words Text Representation\n",
        "####Your Naive Bayes, logistic regression and MLP classifiers will use a bag of words representation for the data. The `torchtext` iterators output tokenized natural language, which you must convert into a bag of words representation. It is highly recommended that you make use of the `Example` class ([documentation found here](https://github.com/pytorch/text/blob/master/torchtext/data/example.py)). In this class, store the text, the label and the bag-of-words representation for each datum in your dataset.\n",
        "---\n",
        "####HINT: Your vocabulary should be derived ONLY from your training set, not your entire dataset. Make certain that your bag-of-words representations account for this. You may have unknown words in your test set and we leave it up to you to decide the best way of handling this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiZRD-ua1Jfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Convert to standard format\n",
        "class ATIS(data.Dataset):\n",
        "    dirname = 'data'\n",
        "    name = 'atis'\n",
        "    _vocab = {}\n",
        "    _labels = []\n",
        "    def __init__(self, path, text_field, label_field, bow_field, vocab, **kwargs):\n",
        "        \"\"\"Create an ATIS dataset instance given a path and fields.\n",
        "        Arguments:\n",
        "            path: Path to the data file\n",
        "            text_field: The field that will be used for text data.\n",
        "            label_field: The field that will be used for label data.\n",
        "            bow_field: The field that will be used for bag-of-words data.\n",
        "            vocab: dictionary mapping vocabulary words to unique indices **Optional**\n",
        "            Remaining keyword arguments: Passed to the constructor of\n",
        "                data.Dataset.\n",
        "        \"\"\"\n",
        "        if (vocab is None): \n",
        "          fields = [('text', text_field), ('label', label_field), ('bow', bow_field)]\n",
        "          examples = []\n",
        "          # Get text\n",
        "          with open(path+'.nl', 'r') as f:\n",
        "            for line in f:\n",
        "              ex = data.Example()\n",
        "              # PyTorch's `preprocess` automatically does spacy tokenization\n",
        "              ex.text = text_field.preprocess(line.strip()) \n",
        "              examples.append(ex)\n",
        "            self._examples = examples\n",
        "          \n",
        "          # Map each vocab word to unique index \n",
        "          word_to_ix = {}\n",
        "          for ex in self._examples:\n",
        "            for word in ex.text:\n",
        "              if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "          self._vocab = word_to_ix\n",
        "          print(\"Vocab Size: {}\".format(len(self._vocab)))\n",
        "\n",
        "          # Get labels & bow representation\n",
        "          with open(path + '.sql', 'r') as f:\n",
        "              for i, line in enumerate(f):\n",
        "                # Get label for text\n",
        "                label = self._get_label_from_query(line.strip())\n",
        "                self._examples[i].label = label\n",
        "                self._labels.append(label)\n",
        "\n",
        "                # Get bag-of-words for text\n",
        "                text = self._examples[i].text\n",
        "                vec = torch.zeros(len(word_to_ix))\n",
        "                for w in text:\n",
        "                  vec[self._vocab[w]] += 1\n",
        "                bow = vec.view(1, -1)\n",
        "                self._examples[i].bow = bow\n",
        "          print(\"Loading dataset {} - {} examples\".format(path, len(self._examples)))\n",
        "          super(ATIS, self).__init__(self._examples, fields, **kwargs)\n",
        "        \n",
        "        else:\n",
        "          words_outside_vocab =  0\n",
        "          fields = [('text', text_field), ('label', label_field), ('bow', bow_field)]\n",
        "          examples = []\n",
        "          # Get text\n",
        "          with open(path+'.nl', 'r') as f:\n",
        "            for line in f:\n",
        "              ex = data.Example()\n",
        "              # PyTorch's `preprocess` automatically does spacy tokenization\n",
        "              ex.text = text_field.preprocess(line.strip()) \n",
        "              examples.append(ex)\n",
        "            self._examples = examples\n",
        "\n",
        "          # Get labels & bow representation\n",
        "          word_to_ix = vocab\n",
        "          with open(path + '.sql', 'r') as f:\n",
        "              for i, line in enumerate(f):\n",
        "                # Get label for text\n",
        "                label = self._get_label_from_query(line.strip())\n",
        "                self._examples[i].label = label\n",
        "\n",
        "                # Get bag-of-words for text\n",
        "                text = self._examples[i].text\n",
        "                vec = torch.zeros(len(word_to_ix))\n",
        "                for w in text:\n",
        "                  if w not in word_to_ix:\n",
        "                    words_outside_vocab += 1\n",
        "                    continue;\n",
        "                  else:\n",
        "                    vec[word_to_ix[w]] += 1\n",
        "                bow = vec.view(1, -1)\n",
        "                self._examples[i].bow = bow\n",
        "\n",
        "          print(\"Loading dataset {} - {} examples\".format(path, len(self._examples)))\n",
        "          print(\"{} words outside of training vocabulary for {} dataset\".format(words_outside_vocab, path))\n",
        "          super(ATIS, self).__init__(self._examples, fields, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return len(ex.text)\n",
        "\n",
        "    # Simple function to get question labels from query\n",
        "    def _get_label_from_query(self, query):\n",
        "        parts = query.split(' ')\n",
        "        if parts[1] == 'DISTINCT':\n",
        "            label = parts[2]\n",
        "        else:\n",
        "            label = parts[1]\n",
        "        \n",
        "        if '.' in label:\n",
        "            label = label.split('.')[-1]\n",
        "        \n",
        "        return label\n",
        "    \n",
        "    # Return labels\n",
        "    def get_labels(self):\n",
        "      return list(set(self._labels))\n",
        "    \n",
        "    # Return vocabulary\n",
        "    def get_vocab(self):\n",
        "      return self._vocab\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, text_field, label_field, bow_field, path='./',\n",
        "               train='train', validation='dev', test='test',\n",
        "               **kwargs):\n",
        "        \"\"\"Create dataset objects for splits of the ATIS dataset.\n",
        "        Arguments:\n",
        "            text_field: The field that will be used for the sentence.\n",
        "            label_field: The field that will be used for label data.\n",
        "            bow_field: The field that will be used for bag-of-words data.\n",
        "            root: The root directory that the dataset's zip archive will be\n",
        "                expanded into; therefore the directory in whose trees\n",
        "                subdirectory the data files will be stored.\n",
        "            train: The filename of the train data. Default: 'train.txt'.\n",
        "            validation: The filename of the validation data, or None to not\n",
        "                load the validation set. Default: 'dev.txt'.\n",
        "            test: The filename of the test data, or None to not load the test\n",
        "                set. Default: 'test.txt'.\n",
        "            Remaining keyword arguments: Passed to the splits method of\n",
        "                Dataset.\n",
        "        \"\"\"\n",
        "        # Get BOW representation for train split.\n",
        "        train_data = None if train is None else cls(\n",
        "            os.path.join(path, train), text_field, label_field, bow_field, None, **kwargs)\n",
        "        vocab = train_data.get_vocab()\n",
        "        labels = train_data.get_labels()\n",
        "\n",
        "        # Use vocabulary to transform val, test splits into BOW.\n",
        "        val_data = None if validation is None else cls(\n",
        "            os.path.join(path, validation), text_field, label_field, bow_field, vocab, **kwargs)\n",
        "        test_data = None if test is None else cls(\n",
        "            os.path.join(path, test), text_field, label_field, bow_field, vocab, **kwargs)\n",
        "        return tuple(d for d in (labels, vocab, train_data, val_data, test_data)\n",
        "                     if d is not None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxSO_FD-4_lg",
        "colab_type": "text"
      },
      "source": [
        "###Implement tortchtext Iterators\n",
        "\n",
        "Next, we create instances of the `data.Field` class for the input (text) and output (labels) fields.\n",
        "The `data.Field` class, include in PyTorch, contains common text-processing datatypes that can be converted to tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0XvgL1X6GiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We set `batch_first` = True to ensure the data is batched before it is processed.\n",
        "TEXT = data.Field(lower=True, include_lengths=False, batch_first=True, tokenize=\"spacy\") \n",
        "LABEL = data.Field(sequential=False, unk_token=None)\n",
        "BOW = data.Field(batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH78WB2o5_nC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We will use the `ATIS.splits` class method to build the `ATIS` instances for train and test data. This method splits the data into either two (train & test) or three (train, test, validation) subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXPI87uI6P_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9ad47ab8-657e-4c3a-8aec-130a8a7dec79"
      },
      "source": [
        "# Make splits for data\n",
        "labels, vocab, train, test = ATIS.splits(TEXT, LABEL, BOW, validation=None)"
      ],
      "execution_count": 662,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 860\n",
            "Loading dataset ./train - 4379 examples\n",
            "Loading dataset ./test - 448 examples\n",
            "29 words outside of training vocabulary for ./test dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go2q9-vd6RO7",
        "colab_type": "text"
      },
      "source": [
        "Once the data is processed we build the vocabulary and then construct iterators which loop over the datasets in batches. This will be important for SGD for logistic regression and for other models later in the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBVa2Krb5IcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make iterator for splits\n",
        "BATCH_SIZE = 32\n",
        "train_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, test), batch_size=BATCH_SIZE, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxDMbHJG9Qpg",
        "colab_type": "text"
      },
      "source": [
        "## Establish a majority baseline\n",
        "\n",
        "By defining a lower bound on performance, we know at minimum what to expect from any reasonable system. A simple baseline for classification tasks is to measure the accuracy of prediction when the most common class is always predicted. \n",
        "\n",
        "**Write code in the cell below that, given train and test data, prints information concerning the majority baseline.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd8XvBof6rVa",
        "colab_type": "code",
        "outputId": "1404cac6-b2b7-4a46-8c8e-3e11ebb2e194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "def majority_baseline_accuracy(train, test):\n",
        "  # Find majority on training data\n",
        "  counts = Counter()\n",
        "  for ex in train:\n",
        "      counts[ex.label] += 1\n",
        "\n",
        "  most_common = counts.most_common(1)[0][0]\n",
        "  print(\"Most common label: {}\".format(most_common))\n",
        "  # Evaluate accuracy on test data\n",
        "  test_counts = Counter()\n",
        "  for ex in test:\n",
        "      test_counts[ex.label] += 1\n",
        "\n",
        "  total_count = len(list(test_counts.elements()))\n",
        "  most_common_count = test_counts[most_common]\n",
        "  print('Count of most common label:', most_common_count)\n",
        "  print('Count of total things labelled:', total_count)\n",
        "  print('Portion of labels that are the most common one:', most_common_count/total_count)\n",
        "\n",
        "majority_baseline_accuracy(train, test)"
      ],
      "execution_count": 626,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common label: flight_id\n",
            "Count of most common label: 306\n",
            "Count of total things labelled: 448\n",
            "Portion of labels that are the most common one: 0.6830357142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNbq_QvG_XGY",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "\n",
        "Naive Bayes classification is based on the \"naive\" assumption that all features are independent. This dramatically reduces the number of parameters required for Bayesian classification, which utilizes Bayes' Theorem which utilizes known information ($P(Y)$, $P(X)$ and $(P(X_i|Y)$) to obtain the desired unknown probability of $P(Y|X)$. This is evaluated for each possible label and the label with greatest likelihood is the prediction for a given text. \n",
        "\n",
        "---\n",
        "Let $ c_{NB} $ be the maximum value in a vector containing the conditional probabilities of label $c$ given each word in the text. Then, we can compute $c_{NB}$ by evaluating the probability of the label overall and the probability of the label given the presence of each word contained in a given text, as \n",
        "$$ c_{NB} = \\text{argmax}_{c \\in C} \\left( \\log P(c) + \\sum_{w \\in W}\\log P(w|c) \\right) $$\n",
        "\n",
        "Where $c_{NB}$ is the naive Bayes classification of a bag of words, $C$ is the set of classifications, and $W$ is the bag of words.\n",
        "\n",
        "We can calculate $P(c) = \\frac{N_c}{N}$ where $N$ is the total number of data points in our training data and $N_c$ is the total number of data points in our training data with classification $c$. \n",
        "\n",
        "We can calculate $P(w_0 | c)$ using Laplace smoothing such that $$P(w_0 | c) = \\frac{count(w_0, c) + 1}{\\left( \\sum_{w \\in V} count(w,c)\\right) + |V|}$$ where $V$ is the vocabulary.\n",
        "\n",
        "----\n",
        "##Below, implement the NaiveBayes class methods.\n",
        " \n",
        "\n",
        "###1.  `train`: Populates the log probabilities table to contain $log(P(c))$ and $log(𝑃(𝑤_i|𝑐)) $ for each label for each word in the vocabulary.**\n",
        "###2.   `evaluate_performance`: Evaluates the performance of the model on given datset and prints accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwSLwSEO2uyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NaiveBayes():\n",
        "    def __init__ (self, texts, bows, labels, vocab):\n",
        "      self.texts = texts\n",
        "      self.bows = bows\n",
        "      self.labels = labels\n",
        "      self.vocab = vocab\n",
        "      self.log_probs = {}\n",
        "    \n",
        "    def train(self, train):\n",
        "      \"\"\"\n",
        "      Populates log probabilities table for training data.\n",
        "      \"\"\"\n",
        "      for label in self.labels:\n",
        "        self.log_probs[label] = {}\n",
        "\n",
        "        # Calculate the log prior (logP(c))\n",
        "        N = len(self.labels)\n",
        "        Nc = sum(example.label == label for example in train.examples)\n",
        "        self.log_probs[label]['log_prior'] = math.log(Nc / N)\n",
        "\n",
        "        # Calculate the log likelyhood (logP(w | c)) for all words in vocab for each label\n",
        "        self.log_probs[label]['log_likelihood'] = {}\n",
        "        for word in vocab:\n",
        "          count_wc = 0\n",
        "          sum_count_wc = 0\n",
        "          for example in train.examples:\n",
        "            if example.label == label:\n",
        "              count_wc += sum(token == word for token in example.text)\n",
        "              sum_count_wc += len(example.text)\n",
        "          Pwc = (count_wc + 1) / (sum_count_wc + len(vocab))\n",
        "          self.log_probs[label]['log_likelihood'][word] = math.log(Pwc)\n",
        "    \n",
        "    def evaluate_performance(self, dataset):\n",
        "      \"\"\"\n",
        "      Takes a dataset and prints the model's performance that dataset.\n",
        "      \"\"\"\n",
        "      # Count the number of correct guesses\n",
        "      correct_guesses = 0\n",
        "      for example in dataset.examples:\n",
        "       # For each example, find the score of each label \n",
        "        scores = {}\n",
        "        for label in self.log_probs:\n",
        "          class_prediction = self.log_probs[label]['log_prior']\n",
        "          for word in example.text:\n",
        "            if word in vocab:\n",
        "              class_prediction += self.log_probs[label]['log_likelihood'][word]\n",
        "          scores[label] = class_prediction\n",
        "\n",
        "        # Find the maximum score to determine our guess for the label\n",
        "        argmax = max(scores, key=scores.get)\n",
        "\n",
        "        # If it matches the actual label, we guessed correctly!\n",
        "        if argmax == dataset.examples[dataset.examples.index(example)].label:\n",
        "          correct_guesses += 1\n",
        "\n",
        "      # Print our accuracy\n",
        "      print('Accuracy: ', correct_guesses / len(dataset.examples))\n",
        "      return correct_guesses/len(dataset.examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1R0Ed5IUqm",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "If you have implemented the class methods, the following should result in a trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCfg-kyV56Kw",
        "colab_type": "code",
        "outputId": "922f3af1-e229-43f7-d23a-364fb96dd644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Bag-of-words vectors for each input query\n",
        "# batch = next(iter(train_iter))\n",
        "\n",
        "# Instantiate and train classifier\n",
        "nb_classifier = NaiveBayes(TEXT, BOW, labels, vocab)\n",
        "nb_classifier.train(train)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Train: \")\n",
        "classifier.evaluate_performance(train)\n",
        "print(\"Test: \")\n",
        "classifier.evaluate_performance(test)"
      ],
      "execution_count": 667,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: \n",
            "Accuracy:  0.8942680977392099\n",
            "Test: \n",
            "Accuracy:  0.84375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 667
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pH4ph3uHnvD",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Unlike Naive Bayes, logistic regression calculates the conditional probabilities directly. If we let $c\\in C$ be a label, $\\mathbf{w} \\in W$ be a bag-of-words representation of a natural language query, $\\mathbf{d}$ be weights in the model tied to the compatability of $c$ and $\\mathbf{w}$, and $f$ is $\\mathbf{d}^T \\mathbf{w}$, we use the softmax to get: \n",
        "$$ p(c|\\mathbf{w}, \\mathbf{d})= \\frac{\\exp (f(\\mathbf{w},c,\\mathbf{d}))}{\\sum_{c'\\in C}\\exp(f(\\mathbf{w},c',\\mathbf{d}))}. $$\n",
        "\n",
        "The weights are learned in the process of training by using a loss function--here the cross entropy loss--to compare the results produced by the current version of the model and the target results. \n",
        "---\n",
        "##Below, implement the LogisticRegression class methods.\n",
        " \n",
        "\n",
        "###1.  `train`: Trains the model for n epochs with provided optimizer and learning rate.\n",
        "###2.   `evaluate_performance`: Evaluates the performance of the model on given datset and prints accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBdFcvg-PYBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Implement the initialization and forward step of the Logistic Regression model.\n",
        "'''\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__ (self, count_labels, bag_size):\n",
        "        super (LogisticRegression, self).__init__ ()\n",
        "        # Linear layer\n",
        "        self.fc = nn.Linear(bag_size, count_labels)\n",
        "        # Bias\n",
        "        self.bias = torch.zeros(count_labels, requires_grad=True).to(device)\n",
        "        \n",
        "    def forward (self, input):\n",
        "        # Apply the linear layer\n",
        "        output = self.fc(input)\n",
        "        # Add the bias and output the result\n",
        "        output = (output + self.bias)\n",
        "        return output\n",
        "\n",
        "    '''\n",
        "    Takes criterion, optimizer and # of epochs, and trains the model on the given data.\n",
        "    '''\n",
        "    def train (self, bow, criterion, optim, n_epochs = 8):\n",
        "      for epoch in range (n_epochs):\n",
        "        c_num = 0\n",
        "        total = 0\n",
        "        for index, batch in enumerate(bow):\n",
        "          print(\"index: {} batch: {}\\n\".format(index, batch))\n",
        "          # Initialize the optimizer\n",
        "          optim.zero_grad()\n",
        "          \n",
        "          # Input and target\n",
        "          input = input2bow(batch.text, len(TEXT.vocab), TEXT.vocab.stoi['<pad>'])\n",
        "          target = batch.label.long()\n",
        "          \n",
        "          # Feed the input and hidden state to the model\n",
        "          scores = self(input)\n",
        "\n",
        "          # Compute the loss\n",
        "          loss = criterion(scores, target)\n",
        "          \n",
        "          # Perform backpropogation\n",
        "          loss.backward()\n",
        "          optim.step()\n",
        "          \n",
        "          # Prepare to compute the accuracy\n",
        "          predictions = torch.argmax(scores, dim=1)\n",
        "          total += len(target)\n",
        "          c_num += (predictions == target).sum().item()\n",
        "\n",
        "          # Report the loss every 200 steps\n",
        "          if index % 200 == 0:\n",
        "              print ('Epoch :', epoch,\n",
        "                    'Step: ', index,\n",
        "                    'Loss: ', loss.item(),\n",
        "                    'Accuracy:', float (c_num)/total)\n",
        "\n",
        "    '''\n",
        "    Takes a model & dataset, and returns accuracy of model on dataset.\n",
        "    '''\n",
        "    def evaluate_performance(self, dataset):\n",
        "        # c_num = 0\n",
        "        # total = 0\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #   data_iter = iter(dataset)\n",
        "        #   for index, batch in enumerate(data_iter):\n",
        "        #       # Input and target\n",
        "        #       input = input2bow(batch.text, len(TEXT.vocab), TEXT.vocab.stoi['<pad>'])\n",
        "        #       target = batch.label.long()\n",
        "\n",
        "        #       # Feed the input and hidden state to the model \n",
        "        #       # then determine the index of the maximum value for each test item\n",
        "        #       scores = self(input)\n",
        "        #       predictions = torch.argmax(scores, dim=1)\n",
        "  \n",
        "        #       # Prepare to compute the accuracy\n",
        "        #       total += len(target)\n",
        "        #       print(predictions == target)\n",
        "        #       c_num += (predictions == target).sum().item()\n",
        "\n",
        "        # # Return the accuracy\n",
        "        # return float (c_num)/total\n",
        "          c_num = 0\n",
        "          total = 0\n",
        "          \n",
        "          # Turn on eval mode\n",
        "          # model.eval ()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for index, batch in enumerate(test_iter):\n",
        "                  # Input and target\n",
        "                  input = input2bow(batch.text, len(TEXT.vocab), TEXT.vocab.stoi['<pad>'])\n",
        "                  target = batch.label.long()\n",
        "\n",
        "                  # Feed the input and hidden state to the model then determine the \n",
        "                        # index of the maximum value for each test item\n",
        "                  scores = model(input)\n",
        "                  predictions = torch.argmax(scores, dim=1)\n",
        "      \n",
        "                  # Prepare to compute the accuracy\n",
        "                  total += len(target)\n",
        "                  c_num += (predictions == target).sum().item()\n",
        "\n",
        "          # Return the accuracy\n",
        "          return float (c_num)/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8UkUosAu9YU",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "If you have implemented the LogisticRegression class methods, the following should result in a trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL2ghDuiu8gR",
        "colab_type": "code",
        "outputId": "385e3eb4-b4ff-45d3-bcca-c136aaf0c2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "# Bag-of-words vectors for each input query\n",
        "batch_bow = input2bow(batch.text, len(TEXT.vocab) - 1, TEXT.vocab.stoi['<pad>'])\n",
        "print(len(batch.text))\n",
        "\n",
        "# Instantiate classifier\n",
        "#Subtract 1 because the bag of words representation removes the padding\n",
        "linear_regression_model = LogisticRegression(LABEL, TEXT).to(device) \n",
        "print(linear_regression_model)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train classifier model on training split\n",
        "linear_regression_model.train(train_iter, loss, optimizer)\n",
        "\n",
        "# Evaluate model performance on training, test splits\n",
        "print(\"Train:\")\n",
        "linear_regression_model.evaluate_performance(train_iter)\n",
        "print(\"Test:\")\n",
        "linear_regression_model.evaluate_performance(test_iter)"
      ],
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-351-ac1a5413a6f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Instantiate classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Subtract 1 because the bag of words representation removes the padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-348-0c9c429e980f>\u001b[0m in \u001b[0;36minput2bow\u001b[0;34m(batch, vocab_size, pad_idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Remove padding token from BoW representation, as it doesn't contain info about the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbatch_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_bow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_bow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_bow shape: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_bow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te35cWGOJlf9",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "An MLP is composed of at least three fully connected layers of nodes, referred to as the input, output and \"hidden\" layers. Learning occurs by adjusting the connection weights between nodes based on the amount of error in the output compared to the prediction. \n",
        "---\n",
        "Let the degree of error in an output node $j$ in the $n$th training query be $e_j(n) = d_j(n) - y_j(n)$, where $d$ is the true label and $y$ is the predicted label.\n",
        "\n",
        "Then we can adjust the weights to minimize the entire output layer's cumulative error:\n",
        "$$\\mathcal{E}(n)=\\frac{1}{2}\\sum_j e_j^2(n)$$\n",
        "\n",
        "The change in each weight according to gradient descent is \n",
        "$$\\Delta w_{ji} (n) = -\\eta\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} y_i(n)$$.\n",
        "\n",
        "---\n",
        "##Implement the methods of the class MultiLayerPerceptron below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1R9xjg5Jnih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class MultiLayerPerceptron(nn.Module):\n",
        "  def __init__(self, label, text, n_hidden=128):\n",
        "    super(MultiLayerPerceptron, self).__init__()\n",
        "    self.label = label\n",
        "    self.text = text\n",
        "    self.input2hidden = nn.Linear(len(self.text.vocab), n_hidden)\n",
        "    print(self.label.vocab.itos)\n",
        "    print(\"len labels: {}\".format(self.label.vocab.itos))\n",
        "    self.hidden2output = nn.Linear(n_hidden, len(self.label.vocab.itos))\n",
        "    self.softmax = nn.LogSoftmax()\n",
        "    self.learning_rate = 0.01\n",
        "    self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
        "    print(\"length of text.vocab = {}\".format(len(self.text.vocab)))\n",
        "\n",
        "  def forward(self, data):\n",
        "    hidden = self.input2hidden(data)\n",
        "    output = self.hidden2output(hidden)\n",
        "    output = self.softmax(output)\n",
        "    return output\n",
        "\n",
        "  def train(self, bow, n_epochs):\n",
        "      for epoch in range (n_epochs):\n",
        "        curr_loss = 0.0\n",
        "        c_num = 0\n",
        "        total = 0\n",
        "        for index, batch in enumerate(bow):\n",
        "          \n",
        "          # Zero the parameter gradients\n",
        "          self.optimizer.zero_grad()\n",
        "          \n",
        "          # Input and target\n",
        "          input = input2bow(batch.text, len(self.text.vocab), self.text.vocab.stoi['<pad>'])\n",
        "          target = batch.label.long()\n",
        "          print(batch.label.long())\n",
        "          print(\"len text vocab {}\".format(len(self.text.vocab)))\n",
        "          print(\"len label vocab {}\".format(len(self.label.vocab)))\n",
        "          \n",
        "          # Forward step\n",
        "          predictions = self(input)\n",
        "          criterion = nn.NLLLoss()\n",
        "\n",
        "          # Compute loss\n",
        "          loss = criterion(predictions, target)\n",
        "          total += len(target)\n",
        "          c_num += (predictions == target).sum().item()\n",
        "          \n",
        "          # Backward step\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Report the loss every 200 steps\n",
        "          curr_loss += loss.item()\n",
        "          if index % 200 == 0:\n",
        "              print ('Epoch :', epoch,\n",
        "                    'Step: ', index,\n",
        "                    'Loss: ', loss.item(),\n",
        "                    'Accuracy:', float (c_num)/total)\n",
        "\n",
        "  def evaluate_performance(self, data):\n",
        "    c_num = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for index, batch in enumerate(data):\n",
        "        # Input and target\n",
        "        input = input2bow(batch.text, len(self.text.vocab) - 1, self.text.vocab.stoi['<pad>'])\n",
        "        target = batch.label.long()\n",
        "\n",
        "        # Feed the input and hidden state to the model then determine the \n",
        "        # index of the maximum value for each test item\n",
        "        scores = self(input)\n",
        "        predictions = torch.argmax(scores, dim=1)\n",
        "\n",
        "        # Prepare to compute the accuracy\n",
        "        total += len(target)\n",
        "        c_num += (predictions == target).sum().item()\n",
        "\n",
        "        # Return the accuracy\n",
        "        print(\"Accuracy: {}\".format(float(c_num)/total))\n",
        "        return float (c_num)/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgmTXLMJP7CF",
        "colab_type": "code",
        "outputId": "7d5f7ac4-34c7-4422-d00d-2426505df38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        }
      },
      "source": [
        "# Bag-of-words vectors for each input query\n",
        "batch_bow = input2bow(batch.text, len(TEXT.vocab), TEXT.vocab.stoi['<pad>'])\n",
        "\n",
        "# Instantiate classifier\n",
        "mlp_classifier = MultiLayerPerceptron(LABEL, TEXT).to(device)\n",
        "print(mlp_classifier)\n",
        "\n",
        "# Train classifier model on training split\n",
        "mlp_classifier.train(batch_bow, 5)\n",
        "\n",
        "# Evaluate model performance on training, test splits\n",
        "print(\"Train:\")\n",
        "mlp_classifier.evaluate_performance(train_iter)\n",
        "print(\"Test:\")\n",
        "mlp_classifier.evaluate_performance(test_iter)"
      ],
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 862 Batch X: 32 Y: 21\n",
            "Rows in Batch: 32\n",
            "batch_bow shape: torch.Size([32, 861])\n",
            "['flight_id', 'fare_id', 'transport_type', 'airline_code', 'aircraft_code', 'departure_time', 'fare_basis_code', 'airport_code', 'count', 'state_code', 'booking_class', 'ground_fare', 'restriction_code', 'arrival_time', 'miles_distant', 'city_code', 'meal_code', 'advance_purchase', 'basic_type', 'flight_number', 'meal_description', 'minutes_distant', 'airport_location', 'time_elapsed', 'day_name', 'stop_airport', 'stops', 'city_name', 'minimum_connect_time', 'time_zone_code']\n",
            "len labels: ['flight_id', 'fare_id', 'transport_type', 'airline_code', 'aircraft_code', 'departure_time', 'fare_basis_code', 'airport_code', 'count', 'state_code', 'booking_class', 'ground_fare', 'restriction_code', 'arrival_time', 'miles_distant', 'city_code', 'meal_code', 'advance_purchase', 'basic_type', 'flight_number', 'meal_description', 'minutes_distant', 'airport_location', 'time_elapsed', 'day_name', 'stop_airport', 'stops', 'city_name', 'minimum_connect_time', 'time_zone_code']\n",
            "length of text.vocab = 862\n",
            "MultiLayerPerceptron(\n",
            "  (input2hidden): Linear(in_features=862, out_features=128, bias=True)\n",
            "  (hidden2output): Linear(in_features=128, out_features=30, bias=True)\n",
            "  (softmax): LogSoftmax()\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-429-bf125e854cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train classifier model on training split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmlp_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Evaluate model performance on training, test splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-428-4458c89cf060>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, bow, n_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0;31m# Input and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m           \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHJe_pKGCWHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}