{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "student_classification_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmarogge/cs280r/blob/master/student_classification_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyMdovPAKAHU",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2: Text classification with Colab and PyTorch\n",
        "\n",
        "Emma Rogge, Tasha Schoenstein & Zilin Ma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKCmwuSH0acV",
        "colab_type": "text"
      },
      "source": [
        "## Set up\n",
        "\n",
        "###Import relevant libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdi-spgB0sEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchtext import data\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "## GPU check, make sure to set runtime type to \"GPU\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "962cJXPYTKyS",
        "colab_type": "text"
      },
      "source": [
        "### Read in data files from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGVWcvlk080Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/train.nl\n",
        "!wget https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/train.sql\n",
        "!wget https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/test.nl\n",
        "!wget https://raw.githubusercontent.com/sriniiyer/nl2sql/master/data/atis/test.sql"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPJ6ihGH1Oz8",
        "colab_type": "text"
      },
      "source": [
        "##Data format\n",
        "\n",
        "We're going to use `torchtext` to handle processing the data. This library is useful for processing and batching text data in Python. More information on `torchtext` can be found [in this tutorial](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBIuSrjYUjVF",
        "colab_type": "text"
      },
      "source": [
        "To begin, we set up two instances of the PyTorch class  one for the natural-language queries and one for the SQL query intent labels.\n",
        "Next, we create instances of the PyTorch [`data.Field`](https://torchtext.readthedocs.io/en/latest/data.html#fields) class for the input (text) and output (labels) fields.\n",
        "This class contains common text-processing datatypes that can be converted to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0XvgL1X6GiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We set `batch_first` = True to ensure the data is batched before it is processed.\n",
        "TEXT = data.Field(lower=True, sequential=True, include_lengths=False, batch_first=True, tokenize=\"spacy\") \n",
        "LABEL = data.Field(batch_first=True, sequential=False, unk_token=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPdPKDzo-Vf6",
        "colab_type": "text"
      },
      "source": [
        "###Implement torchtext Dataset\n",
        "Implement the class below to prepare the data for classification. It is highly recommended that you make use of the [`Example class`](https://github.com/pytorch/text/blob/master/torchtext/data/example.py) to store each corresponding text and label.\n",
        "\n",
        "#### Hints:\n",
        "- Start by populating a list with each processed, tokenized query in your dataset.\n",
        "- Each text field object in your list should then have its `label` field populated with the appropriate label.\n",
        "- Leverage the `__init__` method of the parent class, [`pytorch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset).\n",
        "- Utilize the PyTorch's `preprocess` method on your text field as you iterate--this method tokenizes the content of the 'text' field in an Example using `spacy`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiZRD-ua1Jfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    ## Convert to standard format\n",
        "class ATIS(data.Dataset):\n",
        "    dirname = 'data'\n",
        "    name = 'atis'\n",
        "\n",
        "    def __init__(self, path, text_field, label_field, **kwargs):\n",
        "        \"\"\"Create an ATIS dataset instance given a path and fields.\n",
        "        Arguments:\n",
        "            path: Path to the data file\n",
        "            text_field: The field that will be used for text data.\n",
        "            label_field: The field that will be used for label data.\n",
        "            Remaining keyword arguments: Passed to the constructor of\n",
        "                data.Dataset.\n",
        "        \"\"\"\n",
        "        super(ATIS, self).__init__(examples, fields, **kwargs)\n",
        "        #TODO: Complete the implementation of this method.\n",
        "\n",
        "\n",
        "    # Simple function to get question labels from query\n",
        "    def _get_label_from_query(self, query):\n",
        "        #TODO: Implement this method.\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, text_field, label_field, path='./',\n",
        "               train='train', validation='dev', test='test',\n",
        "               **kwargs):\n",
        "        \"\"\"Create dataset objects for splits of the ATIS dataset.\n",
        "        Arguments:\n",
        "            text_field: The field that will be used for the sentence.\n",
        "            label_field: The field that will be used for label data.\n",
        "            root: The root directory that the dataset's zip archive will be\n",
        "                expanded into; therefore the directory in whose trees\n",
        "                subdirectory the data files will be stored.\n",
        "            train: The filename of the train data. Default: 'train.txt'.\n",
        "            validation: The filename of the validation data, or None to not\n",
        "                load the validation set. Default: 'dev.txt'.\n",
        "            test: The filename of the test data, or None to not load the test\n",
        "                set. Default: 'test.txt'.\n",
        "            Remaining keyword arguments: Passed to the splits method of\n",
        "                Dataset.\n",
        "        \"\"\"\n",
        "        train_data = None if train is None else cls(\n",
        "            os.path.join(path, train), text_field, label_field, **kwargs)\n",
        "        val_data = None if validation is None else cls(\n",
        "            os.path.join(path, validation), text_field, label_field, **kwargs)\n",
        "        test_data = None if test is None else cls(\n",
        "            os.path.join(path, test), text_field, label_field, **kwargs)\n",
        "        return tuple(d for d in (train_data, val_data, test_data)\n",
        "                     if d is not None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxSO_FD-4_lg",
        "colab_type": "text"
      },
      "source": [
        "###Implement tortchtext Iterators\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH78WB2o5_nC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We will use the `ATIS.splits` class method to build the `ATIS` instances for train and test data. This method splits the data into either two (train & test) or three (train, test, validation) subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXPI87uI6P_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make splits for data\n",
        "train_data, test_data = ATIS.splits(TEXT, LABEL, validation=None)\n",
        "\n",
        "# Build vocabulary for data fields\n",
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go2q9-vd6RO7",
        "colab_type": "text"
      },
      "source": [
        "Once the data is processed we build the vocabulary and then construct iterators which loop over the datasets in batches. This will be important for SGD for logistic regression and for other models later in the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBVa2Krb5IcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make iterator for splits\n",
        "BATCH_SIZE = 32\n",
        "train_iter = data.BucketIterator(\n",
        "    train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device)\n",
        "\n",
        "test_iter = data.Iterator(test_data, batch_size=BATCH_SIZE, sort=False, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4LNuNJChQaR",
        "colab_type": "text"
      },
      "source": [
        "### Bag-of-Words Text Representation\n",
        "####Your Naive Bayes, logistic regression and MLP classifiers will use a bag of words representation for the data. The `torchtext` iterators output tokenized natural language, which you must convert into a bag of words representation. \n",
        "---\n",
        "####HINT: Your vocabulary should be derived ONLY from your training set, not your entire dataset. Make certain that your bag-of-words representations account for this. You may have unknown words in your test set and we leave it up to you to decide the best way of handling this.\n",
        "\n",
        "Additionally, you may find the following methods for tensor manipulation useful to ensure your tensors are of the appropriate dimensions.\n",
        "\n",
        "\n",
        "\n",
        "* https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
        "* https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_\n",
        "* https://pytorch.org/docs/stable/torch.html#torch.sum\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAqr0E1ThPx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute size of vocabulary\n",
        "vocab_size = len(TEXT.vocab.itos)\n",
        "labels_size = len(LABEL.vocab.itos)\n",
        "print(\"Size of vocab: {}\".format(vocab_size))\n",
        "\n",
        "# Given a batch, provide a bag-of-words vector.\n",
        "def batch_to_bow(batch, vocab_size):\n",
        "    # TODO: Implement this method.\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxDMbHJG9Qpg",
        "colab_type": "text"
      },
      "source": [
        "## Establish a majority baseline\n",
        "\n",
        "By defining a lower bound on performance, we know at minimum what to expect from any reasonable system. A simple baseline for classification tasks is to measure the accuracy of prediction when the most common class is always predicted. \n",
        "\n",
        "**Write code in the cell below that, given train and test data, prints information concerning the majority baseline.**\n",
        "\n",
        "HINT: Use the [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) class from Python's `collections` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd8XvBof6rVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def majority_baseline_accuracy(train, test):\n",
        "    #TODO: Implement this method as described.\n",
        "    return None\n",
        "\n",
        "# Call the method to establish a baseline\n",
        "majority_baseline_accuracy(train_data, test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNbq_QvG_XGY",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "\n",
        "Naive Bayes classification is based on the \"naive\" assumption that all features are independent. This dramatically reduces the number of parameters required for Bayesian classification, which utilizes Bayes' Theorem which utilizes known information ($P(Y)$, $P(X)$ and $(P(X_i|Y)$) to obtain the desired unknown probability of $P(Y|X)$. This is evaluated for each possible label and the label with greatest likelihood is the prediction for a given text. \n",
        "\n",
        "---\n",
        "Let $ c_{NB} $ be the maximum value in a vector containing the conditional probabilities of label $c$ given each word in the text. Then, we can compute $c_{NB}$ by evaluating the probability of the label overall and the probability of the label given the presence of each word contained in a given text, as \n",
        "$$ c_{NB} = \\text{argmax}_{c \\in C} \\left( \\log P(c) + \\sum_{w \\in W}\\log P(w|c) \\right) $$\n",
        "\n",
        "Where $c_{NB}$ is the naive Bayes classification of a bag of words, $C$ is the set of classifications, and $W$ is the bag of words.\n",
        "\n",
        "We can calculate $P(c) = \\frac{N_c}{N}$ where $N$ is the total number of data points in our training data and $N_c$ is the total number of data points in our training data with classification $c$. \n",
        "\n",
        "We can calculate $P(w_0 | c)$ using Laplace smoothing such that $$P(w_0 | c) = \\frac{count(w_0, c) + 1}{\\left( \\sum_{w \\in V} count(w,c)\\right) + |V|}$$ where $V$ is the vocabulary.\n",
        "\n",
        "----\n",
        "##Below, implement the NaiveBayes class methods.\n",
        " \n",
        "\n",
        "###1.  `train`: Populates the log probabilities table to contain $log(P(c))$ and $log(ùëÉ(ùë§_i|ùëê)) $ for each label for each word in the vocabulary.\n",
        "###2.   `evaluate_performance`: Evaluates the performance of the model on given datset and prints accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwSLwSEO2uyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NaiveBayes():\n",
        "    def __init__ (self, text, label):\n",
        "        # TODO: Implement this method.\n",
        "        \n",
        "    def train(self, dataset):\n",
        "        \"\"\"\n",
        "        Populates log probabilities table for training data.\n",
        "        \"\"\"\n",
        "        #TODO: Implement this method.\n",
        "    \n",
        "    def evaluate_performance(self, dataset):\n",
        "        \"\"\"\n",
        "        Takes a dataset and returns the model's performance that dataset.\n",
        "        \"\"\"\n",
        "        #TODO: Implement this method."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1R0Ed5IUqm",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "If you have implemented the NaiveBayes class and associated methods, the following code will train the model on the training set and evaluates its performance on both train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCfg-kyV56Kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate and train classifier\n",
        "nb_classifier = NaiveBayes(TEXT, LABEL)\n",
        "nb_classifier.train(train_data)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Train: \")\n",
        "nb_classifier.evaluate_performance(train_data)\n",
        "print(\"Test: \")\n",
        "nb_classifier.evaluate_performance(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pH4ph3uHnvD",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Unlike Naive Bayes, logistic regression calculates the conditional probabilities directly. If we let $c\\in C$ be a label, $\\mathbf{w} \\in W$ be a bag-of-words representation of a natural language query, $\\mathbf{d}$ be weights in the model tied to the compatability of $c$ and $\\mathbf{w}$, and $f$ is $\\mathbf{d}^T \\mathbf{w}$, we use the softmax to get: \n",
        "$$ p(c|\\mathbf{w}, \\mathbf{d})= \\frac{\\exp (f(\\mathbf{w},c,\\mathbf{d}))}{\\sum_{c'\\in C}\\exp(f(\\mathbf{w},c',\\mathbf{d}))}. $$\n",
        "\n",
        "The weights are learned in the process of training by using a loss function--here the cross entropy loss--to compare the results produced by the current version of the model and the target results. \n",
        "---\n",
        "###Below, implement the LogisticRegression class methods.\n",
        "####1.  `__init__` : Takes the TEXT and LABEL `data.Field` instances and initializes the model.\n",
        "####2.  `forward` : Given an input tensor, performs the forward step of the logistic regression.\n",
        "HINT: Your LogisticRegression implementation may inherit from PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#module) class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBdFcvg-PYBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__ (self, label, text):\n",
        "        #TODO: Implement this method.\n",
        "\n",
        "    def forward (self, input):\n",
        "        #TODO: Implement this method."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSIQyAvod0b1",
        "colab_type": "text"
      },
      "source": [
        "### Implement the method `train` for the LogisticRegression model.\n",
        "<b>Parameters:</b> LogisticRegression model, data iterator, criterion, optimizer and # of epochs.\n",
        "\n",
        "Trains the model for n epochs with provided optimizer and learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuDbDkOkdzYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Implement the method.\n",
        "def train (model, data_iter, criterion, optim, n_epochs = 8):\n",
        "    loss_values = []\n",
        "    epochs = []\n",
        "    for epoch in range (n_epochs):\n",
        "        # TODO: Complete the implementation such that the sanity check, \n",
        "        # an 'Epoch vs Loss' plot, is rendered when you run the notebook.\n",
        "        epoch_loss = running_loss / len(train_data)\n",
        "        loss_values.append(epoch_loss)\n",
        "        epochs.append(epoch)\n",
        "\n",
        "    p0 = plt.figure(0)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(epochs, loss_values)\n",
        "    plt.title(\"Epoch vs Loss\")\n",
        "    p0.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hkNzQy-qOZx",
        "colab_type": "text"
      },
      "source": [
        "### Implement the method `evaluate_performance`.\n",
        "This method takes a model & dataset, and returns the accuracy of the model on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFjjzy9QqNw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_performance(model, data_iter):\n",
        "    #TODO: Implement method."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8UkUosAu9YU",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "If you have implemented the LogisticRegression class and associated methods, the following code will  train the model on the training set and evaluates its performance on both train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL2ghDuiu8gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate classifier\n",
        "logreg_model = LogisticRegression(LABEL, TEXT).to(device) \n",
        "print(logreg_model)\n",
        "\n",
        "# Build criterion (loss), optimizer\n",
        "loss = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(logreg_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train classifier model on training split\n",
        "epochs = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "for n in range (5, 15):\n",
        "    # Train model for n epochs\n",
        "    train(logreg_model, train_iter, loss, optimizer, n)\n",
        "    epochs.append(n)\n",
        "    # Evaluate model performance on training, test splits\n",
        "    train_acc.append(evaluate_performance(logreg_model, test_iter))\n",
        "    test_acc.append(evaluate_performance(logreg_model, test_iter))\n",
        "\n",
        "# Graph loss vs accuracy for train, test\n",
        "p1 = plt.figure(1)\n",
        "plt.title(\"Epochs vs Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "train_line = plt.plot(epochs, train_acc, 'b', label=\"Train\")\n",
        "test_line = plt.plot(epochs, test_acc, 'r', label=\"Test\")\n",
        "plt.legend()\n",
        "p1.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te35cWGOJlf9",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "An MLP is composed of at least three fully connected layers of nodes, referred to as the input, output and \"hidden\" layers. Learning occurs by adjusting the connection weights between nodes based on the amount of error in the output compared to the prediction. \n",
        "\n",
        "---\n",
        "Let the degree of error in an output node $j$ in the $n$th training query be $e_j(n) = d_j(n) - y_j(n)$, where $d$ is the true label and $y$ is the predicted label.\n",
        "\n",
        "Then we can adjust the weights to minimize the entire output layer's cumulative error:\n",
        "$$\\mathcal{E}(n)=\\frac{1}{2}\\sum_j e_j^2(n)$$\n",
        "\n",
        "The change in each weight according to gradient descent is \n",
        "$$\\Delta w_{ji} (n) = -\\eta\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} y_i(n)$$.\n",
        "\n",
        "---\n",
        "##Implement the methods of the class MultiLayerPerceptron below.\n",
        "####1.  `__init__` : Takes the TEXT and LABEL `data.Field` instances and initializes the model.\n",
        "####2.  `forward` : Given an input tensor, performs the forward steps of the multilayer perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1R9xjg5Jnih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "    def __init__(self, label, text, n_hidden=128):\n",
        "        #TODO: Implement this method.\n",
        "\n",
        "    def forward(self, input):\n",
        "        #TODO: Implement this method."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pso30lRKXVTb",
        "colab_type": "text"
      },
      "source": [
        "### Implement the method `train` for the MultiLayerPerceptron model.\n",
        "<b>Parameters:</b> MultiLayerPerceptron model, data iterator, criterion, optimizer and # of epochs.\n",
        "\n",
        "Trains the model for n epochs with provided optimizer and learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CK9FDYX01-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data_iter, criterion, optim, n_epochs = 10):\n",
        "    loss_values = []\n",
        "    epochs = []\n",
        "    for epoch in range (n_epochs):\n",
        "        curr_loss = 0.0\n",
        "        running_loss = 0.0\n",
        "        c_num = 0\n",
        "        total = 0\n",
        "        for index, batch in enumerate(data_iter):\n",
        "            # TODO: Complete the implementation such that the sanity check, \n",
        "            # an 'Epoch vs Loss' plot, is rendered when you run the notebook.\n",
        "            epoch_loss = running_loss / len(train_data)\n",
        "            loss_values.append(epoch_loss)\n",
        "            epochs.append(epoch)\n",
        "    p2 = plt.figure(2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(epochs, loss_values)\n",
        "    plt.title(\"Epoch vs Loss\")\n",
        "    p2.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFrkAD4JXZl6",
        "colab_type": "text"
      },
      "source": [
        "### Implement the method `evalaute_performance`.\n",
        "This method takes a model & dataset, and returns the accuracy of the model on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SEL0YI011q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_performance(model, data_iter):\n",
        "    c_num = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for index, batch in enumerate(data_iter):\n",
        "            # Input and target\n",
        "            input = batch_to_bow(batch.text, model.get_vocab_size())\n",
        "            target = batch.label.long()\n",
        "\n",
        "            # Feed the input and hidden state to the model\n",
        "            scores = model(input)\n",
        "            \n",
        "            # Determine the index of the maximum value for each test item\n",
        "            predictions = torch.argmax(scores, dim=1)\n",
        "\n",
        "            # Prepare to compute the accuracy\n",
        "            total += len(target)\n",
        "            c_num += (predictions == target).sum().item()\n",
        "\n",
        "            # Return the accuracy\n",
        "            print(\"Accuracy: {}\".format(float(c_num)/total))\n",
        "            return float (c_num)/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdTRR6sHu0bo",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "If you have implemented the MultiLayerPerceptron class and associated methods, the following code will  train the model on the training set and evaluates its performance on both train and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgmTXLMJP7CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate classifier\n",
        "mlp_model = MultiLayerPerceptron(LABEL, TEXT).to(device)\n",
        "print(mlp_model)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train classifier model on training split\n",
        "epochs = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "for n in range (5, 50, 5):\n",
        "    # Train model for n epochs\n",
        "    train(mlp_model, train_iter, loss, optimizer, n)\n",
        "    epochs.append(n)\n",
        "    # Evaluate model performance on training, test splits\n",
        "    train_acc.append(evaluate_performance(mlp_model, train_iter))\n",
        "    test_acc.append(evaluate_performance(mlp_model, test_iter))\n",
        "\n",
        "# Graph loss vs accuracy for train, test\n",
        "p3 = plt.figure(3)\n",
        "plt.title(\"MLP - Epochs vs Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "train_line = plt.plot(epochs, train_acc, 'b', label=\"Train\")\n",
        "test_line = plt.plot(epochs, test_acc, 'r', label=\"Test\")\n",
        "plt.legend()\n",
        "p3.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}